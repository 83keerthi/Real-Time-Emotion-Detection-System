{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bdbe0c-b0a0-40d0-bfd8-c973c83922c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import threading\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610c234-e293-4bd9-a7bd-041f43f79971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile('RAVDESS.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/RAVDESS')\n",
    "\n",
    "# Define the path to the extracted files\n",
    "extracted_path = '/content/RAVDESS'\n",
    "\n",
    "# Define the emotion labels\n",
    "emotion_labels = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Initialize your lists or other variables as needed\n",
    "X_audio = []\n",
    "y_audio = []\n",
    "\n",
    "# Define fixed length for padding/truncating\n",
    "fixed_length = 173\n",
    "\n",
    "# Iterate through the subdirectories and files in the extracted directory\n",
    "for root, dirs, files in os.walk(extracted_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            segments = file.split('-')\n",
    "            if len(segments) > 2:\n",
    "                try:\n",
    "                    emotion = int(segments[2]) - 1  # Example: 02-01-06-01-02-01-12.wav\n",
    "                    y, sr = librosa.load(file_path, sr=None)\n",
    "                    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "                    \n",
    "                    # Pad or truncate MFCCs to the fixed length\n",
    "                    if mfccs.shape[1] < fixed_length:\n",
    "                        mfccs = np.pad(mfccs, ((0, 0), (0, fixed_length - mfccs.shape[1])), mode='constant')\n",
    "                    else:\n",
    "                        mfccs = mfccs[:, :fixed_length]\n",
    "                    \n",
    "                    X_audio.append(mfccs)\n",
    "                    y_audio.append(emotion)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert lists to numpy arrays for further processing\n",
    "X_audio = np.array(X_audio)\n",
    "y_audio = np.array(y_audio)\n",
    "\n",
    "print(\"Feature extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1dd025-4126-45ef-a3fa-f4cad1418072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved based on emotions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directories for each emotion\n",
    "for emotion_id, emotion_label in emotion_labels.items():\n",
    "    os.makedirs(f'/content/audio_features/{emotion_label}', exist_ok=True)\n",
    "\n",
    "# Save the features in corresponding directories\n",
    "for i, mfcc in enumerate(X_audio):\n",
    "    emotion = emotion_labels[str(y_audio[i] + 1).zfill(2)]\n",
    "    feature_path = f'/content/audio_features/{emotion}/{i}.pkl'\n",
    "    with open(feature_path, 'wb') as f:\n",
    "        pickle.dump(mfcc, f)\n",
    "\n",
    "print(\"Features saved based on emotions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3373a-8116-4f1d-8594-8a8558cab0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "\n",
    "# Reshape data for the CNN input\n",
    "X_audio = X_audio.reshape(X_audio.shape[0], X_audio.shape[1], X_audio.shape[2], 1)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(13, 173, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(emotion_labels), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632469cf-95d3-42e3-93b2-6ad620869140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_audio, y_audio, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91493f71-77d7-49c1-b4bf-401364d2a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and X_test are 4D arrays of shape (num_samples, time_steps, num_features, 1)\n",
    "# We need to reshape the data to 2D for normalization, but we should preserve the time_steps and num_features separately.\n",
    "\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)  # Flatten the time and feature dimensions for normalization\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e525e67-69d3-49a8-8fd3-894fb40d0b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_normalized: (2304, 13, 173)\n",
      "Shape of X_test_normalized: (576, 13, 173)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your data is in the shape (num_samples, time_steps, num_features, 1)\n",
    "# Remove the singleton dimension\n",
    "X_train_reshaped = X_train.squeeze(-1)\n",
    "X_test_reshaped = X_test.squeeze(-1)\n",
    "\n",
    "# Apply normalization to each feature for every time step\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize along the time_steps and features (across each sample)\n",
    "X_train_normalized = np.array([scaler.fit_transform(sample) for sample in X_train_reshaped])\n",
    "X_test_normalized = np.array([scaler.transform(sample) for sample in X_test_reshaped])\n",
    "\n",
    "# Check the new shape of the data\n",
    "print(f\"Shape of X_train_normalized: {X_train_normalized.shape}\")\n",
    "print(f\"Shape of X_test_normalized: {X_test_normalized.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe97ec6-2882-4ac1-9d9c-da33449e9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(13, 173)),  # Explicitly define the input shape using Input layer\n",
    "    LSTM(64, return_sequences=False),  # LSTM layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # or 'softmax' for multi-class classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "818f51ad-aa77-44fe-a966-d84408980f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G KEERTHI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">171</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2624</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">336,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m171\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m85\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m85\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m83\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2624\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m336,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m1,032\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">355,848</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m355,848\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">355,848</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m355,848\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "\n",
    "# Reshape data for the CNN input\n",
    "X_audio = X_audio.reshape(X_audio.shape[0], X_audio.shape[1], X_audio.shape[2], 1)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(13, 173, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(emotion_labels), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7841a482-18c8-49b7-81ba-0db325252dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.1356 - loss: 2.1148 - val_accuracy: 0.1858 - val_loss: 1.9895\n",
      "Epoch 2/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.2484 - loss: 1.8975 - val_accuracy: 0.2708 - val_loss: 1.8707\n",
      "Epoch 3/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.2851 - loss: 1.7787 - val_accuracy: 0.2917 - val_loss: 1.8263\n",
      "Epoch 4/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.3208 - loss: 1.7409 - val_accuracy: 0.3229 - val_loss: 1.7717\n",
      "Epoch 5/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.3525 - loss: 1.6446 - val_accuracy: 0.3490 - val_loss: 1.7351\n",
      "Epoch 6/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.3947 - loss: 1.5935 - val_accuracy: 0.3715 - val_loss: 1.7457\n",
      "Epoch 7/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4093 - loss: 1.5580 - val_accuracy: 0.3663 - val_loss: 1.6761\n",
      "Epoch 8/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4367 - loss: 1.4738 - val_accuracy: 0.3750 - val_loss: 1.6560\n",
      "Epoch 9/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4328 - loss: 1.4578 - val_accuracy: 0.3715 - val_loss: 1.6759\n",
      "Epoch 10/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4489 - loss: 1.4359 - val_accuracy: 0.4045 - val_loss: 1.6148\n",
      "Epoch 11/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4575 - loss: 1.3789 - val_accuracy: 0.4219 - val_loss: 1.5476\n",
      "Epoch 12/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4714 - loss: 1.3509 - val_accuracy: 0.4392 - val_loss: 1.5618\n",
      "Epoch 13/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4821 - loss: 1.3017 - val_accuracy: 0.4410 - val_loss: 1.5138\n",
      "Epoch 14/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5107 - loss: 1.2629 - val_accuracy: 0.4323 - val_loss: 1.5665\n",
      "Epoch 15/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5069 - loss: 1.2277 - val_accuracy: 0.4479 - val_loss: 1.4621\n",
      "Epoch 16/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5235 - loss: 1.2225 - val_accuracy: 0.4514 - val_loss: 1.4687\n",
      "Epoch 17/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.5324 - loss: 1.1990 - val_accuracy: 0.4740 - val_loss: 1.4059\n",
      "Epoch 18/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.5666 - loss: 1.1460 - val_accuracy: 0.4618 - val_loss: 1.4407\n",
      "Epoch 19/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.5828 - loss: 1.0882 - val_accuracy: 0.4896 - val_loss: 1.4341\n",
      "Epoch 20/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5578 - loss: 1.0928 - val_accuracy: 0.5104 - val_loss: 1.3818\n",
      "Epoch 21/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6071 - loss: 1.0192 - val_accuracy: 0.5174 - val_loss: 1.3958\n",
      "Epoch 22/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6333 - loss: 1.0088 - val_accuracy: 0.5521 - val_loss: 1.2506\n",
      "Epoch 23/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6286 - loss: 0.9735 - val_accuracy: 0.5330 - val_loss: 1.2582\n",
      "Epoch 24/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6404 - loss: 0.9779 - val_accuracy: 0.5191 - val_loss: 1.4009\n",
      "Epoch 25/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6449 - loss: 0.9155 - val_accuracy: 0.5382 - val_loss: 1.2932\n",
      "Epoch 26/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6413 - loss: 0.9339 - val_accuracy: 0.5608 - val_loss: 1.2800\n",
      "Epoch 27/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6869 - loss: 0.8135 - val_accuracy: 0.5660 - val_loss: 1.2660\n",
      "Epoch 28/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6741 - loss: 0.8670 - val_accuracy: 0.5208 - val_loss: 1.4429\n",
      "Epoch 29/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6827 - loss: 0.8247 - val_accuracy: 0.5972 - val_loss: 1.2500\n",
      "Epoch 30/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.7151 - loss: 0.7723 - val_accuracy: 0.5972 - val_loss: 1.2169\n",
      "18/18 - 0s - 15ms/step - accuracy: 0.5972 - loss: 1.2169\n",
      "\n",
      "Test accuracy: 0.5972222089767456\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_normalized, y_train, epochs=30, batch_size=32, validation_data=(X_test_normalized, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_normalized, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc7bee-d27d-4499-8a4f-799e03e0c9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0962e09-7def-4b45-82b0-49aad611f490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9ad5c6-d17d-48b2-be46-e6d1bd52d9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('/content/voice_emotion_model.keras')\n",
    "print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ee1cf2-f6eb-4d7c-a512-32df8bb7a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voice_emotion_model saved\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('/content/voice_emotion_model.keras')\n",
    "print(\"voice_emotion_model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37363ce4-1002-4e4b-948d-ca1896663cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sounddevice) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.22)\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b8696f-f4a2-4090-9fbe-3b8dec2780fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to record or type 'exit' to quit:  enter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "Detected Emotion: disgust\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to record or type 'exit' to quit:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Detected Emotion: angry\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to record or type 'exit' to quit:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Detected Emotion: disgust\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to record or type 'exit' to quit:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Detected Emotion: angry\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to record or type 'exit' to quit:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Detected Emotion: happy\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to record or type 'exit' to quit:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Detected Emotion: fearful\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to record or type 'exit' to quit:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Detected Emotion: sad\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to record or type 'exit' to quit:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Detected Emotion: fearful\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to record or type 'exit' to quit:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Define the emotion labels\n",
    "emotion_labels = {\n",
    "    0: 'neutral',\n",
    "    1: 'calm',\n",
    "    2: 'happy',\n",
    "    3: 'sad',\n",
    "    4: 'angry',\n",
    "    5: 'fearful',\n",
    "    6: 'disgust',\n",
    "    7: 'surprised'\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "duration = 3  # seconds\n",
    "fixed_length = 173  # Length of the MFCC features\n",
    "\n",
    "def predict_emotion(audio_data, sample_rate):\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "    # Pad or truncate MFCCs to the fixed length\n",
    "    if mfccs.shape[1] < fixed_length:\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, fixed_length - mfccs.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfccs = mfccs[:, :fixed_length]\n",
    "\n",
    "    # Reshape for the model\n",
    "    mfccs = mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1], 1)\n",
    "\n",
    "    # Predict emotion\n",
    "    predictions = model.predict(mfccs)\n",
    "    emotion_index = np.argmax(predictions)\n",
    "    return emotion_labels[emotion_index]\n",
    "\n",
    "def record_and_predict():\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(duration * 44100), samplerate=44100, channels=1, dtype='float32')\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    audio_data = audio_data.flatten()  # Flatten the audio data to 1D array\n",
    "    emotion = predict_emotion(audio_data, 44100)\n",
    "    print(f\"Detected Emotion: {emotion}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"Press Enter to record or type 'exit' to quit: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting...\")\n",
    "            break  # Exit the loop\n",
    "        record_and_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d19e4-3aba-4729-b8e1-ab720af1e34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a31417-a8ce-49e5-bf11-a19c29f4cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Face emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bb2a8-5b30-4649-ae12-5692a35d0fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d19ba1-6b43-495e-a2c5-9a26bac1d02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y: [0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G KEERTHI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.5743 - loss: 1.4701 - val_accuracy: 0.6740 - val_loss: 1.4023\n",
      "Epoch 2/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.6466 - loss: 1.3006 - val_accuracy: 0.6740 - val_loss: 1.4070\n",
      "Epoch 3/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.6626 - loss: 1.2797 - val_accuracy: 0.6740 - val_loss: 1.3983\n",
      "Epoch 4/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.6499 - loss: 1.2743 - val_accuracy: 0.6740 - val_loss: 1.2767\n",
      "Epoch 5/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.6546 - loss: 1.2381 - val_accuracy: 0.6740 - val_loss: 1.2666\n",
      "Epoch 6/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.6424 - loss: 1.1791 - val_accuracy: 0.6740 - val_loss: 1.1194\n",
      "Epoch 7/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.6537 - loss: 1.1136 - val_accuracy: 0.6906 - val_loss: 1.0379\n",
      "Epoch 8/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.6721 - loss: 1.0551 - val_accuracy: 0.7293 - val_loss: 0.9372\n",
      "Epoch 9/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7139 - loss: 0.9169 - val_accuracy: 0.7735 - val_loss: 0.8047\n",
      "Epoch 10/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7540 - loss: 0.8408 - val_accuracy: 0.7735 - val_loss: 0.7208\n",
      "Epoch 11/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.7727 - loss: 0.7355 - val_accuracy: 0.8122 - val_loss: 0.6418\n",
      "Epoch 12/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7911 - loss: 0.7027 - val_accuracy: 0.8066 - val_loss: 0.6308\n",
      "Epoch 13/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7801 - loss: 0.7058 - val_accuracy: 0.8398 - val_loss: 0.5718\n",
      "Epoch 14/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8101 - loss: 0.6574 - val_accuracy: 0.8453 - val_loss: 0.5395\n",
      "Epoch 15/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8215 - loss: 0.5887 - val_accuracy: 0.8453 - val_loss: 0.5035\n",
      "Epoch 16/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8229 - loss: 0.5947 - val_accuracy: 0.8508 - val_loss: 0.4928\n",
      "Epoch 17/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8385 - loss: 0.5473 - val_accuracy: 0.8508 - val_loss: 0.4937\n",
      "Epoch 18/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8345 - loss: 0.5693 - val_accuracy: 0.8619 - val_loss: 0.4628\n",
      "Epoch 19/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8438 - loss: 0.5166 - val_accuracy: 0.8564 - val_loss: 0.4532\n",
      "Epoch 20/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8178 - loss: 0.5587 - val_accuracy: 0.8508 - val_loss: 0.4345\n",
      "Epoch 21/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8687 - loss: 0.4261 - val_accuracy: 0.8674 - val_loss: 0.4213\n",
      "Epoch 22/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8414 - loss: 0.4894 - val_accuracy: 0.8674 - val_loss: 0.3900\n",
      "Epoch 23/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8659 - loss: 0.4432 - val_accuracy: 0.8674 - val_loss: 0.3897\n",
      "Epoch 24/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8748 - loss: 0.3729 - val_accuracy: 0.8785 - val_loss: 0.3698\n",
      "Epoch 25/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8787 - loss: 0.3812 - val_accuracy: 0.8785 - val_loss: 0.3648\n",
      "Epoch 26/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8918 - loss: 0.3780 - val_accuracy: 0.8674 - val_loss: 0.3627\n",
      "Epoch 27/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8852 - loss: 0.3476 - val_accuracy: 0.8564 - val_loss: 0.4047\n",
      "Epoch 28/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8976 - loss: 0.3422 - val_accuracy: 0.8895 - val_loss: 0.3328\n",
      "Epoch 29/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8668 - loss: 0.3796 - val_accuracy: 0.8564 - val_loss: 0.3862\n",
      "Epoch 30/30\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8911 - loss: 0.3232 - val_accuracy: 0.8895 - val_loss: 0.3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_emotion_model saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    pixels = row['pixels'].split(' ')\n",
    "    X.append(np.array(pixels, 'float32'))\n",
    "    y.append(row['emotion'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Check unique labels\n",
    "unique_labels = np.unique(y)\n",
    "print(\"Unique labels in y:\", unique_labels)\n",
    "\n",
    "# Ensure labels are within the expected range (0-6)\n",
    "valid_indices = [i for i, label in enumerate(y) if label >= 0 and label <= 6]\n",
    "X = X[valid_indices]\n",
    "y = y[valid_indices]\n",
    "\n",
    "# Normalize the data\n",
    "X = X / 255.0\n",
    "X = X.reshape(X.shape[0], 48, 48, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y = to_categorical(y, num_classes=7)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "model.save('/content/face_emotion_model.h5')\n",
    "print(\"face_emotion_model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c5d30-5d42-48c7-a0ff-972c15d4dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained face detection model and emotion recognition model\n",
    "face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "emotion_model = load_model('face_emotion_model.h5')\n",
    "\n",
    "# Emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the region of interest (ROI) and preprocess it\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48))\n",
    "        roi_gray = roi_gray.astype('float32') / 255.0\n",
    "        roi_gray = np.reshape(roi_gray, (1, 48, 48, 1))\n",
    "        \n",
    "        # Predict emotion\n",
    "        prediction = emotion_model.predict(roi_gray)\n",
    "        max_index = np.argmax(prediction[0])\n",
    "        predicted_emotion = emotion_labels[max_index]\n",
    "        \n",
    "        # Draw a rectangle around the face and label the emotion\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, predicted_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "    \n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d41c38-c3b2-4ce2-ad78-b5de073d9a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.14.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (70.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: rich in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\g keerthi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install librosa tensorflow numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e6b74-1b3c-4e77-a8b4-6fdadb443306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Check if the model files exist\n",
    "if not os.path.exists('/content/voice_emotion_model.keras'):\n",
    "    raise FileNotFoundError(\"Voice emotion model file not found.\")\n",
    "if not os.path.exists('/content/face_emotion_model.h5'):\n",
    "    raise FileNotFoundError(\"Face emotion model file not found.\")\n",
    "\n",
    "# Load the saved models\n",
    "voice_model = tf.keras.models.load_model('/content/voice_emotion_model.keras')\n",
    "face_model = tf.keras.models.load_model('/content/face_emotion_model.h5')\n",
    "\n",
    "# Compile the models (if needed)\n",
    "voice_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "face_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Models loaded and compiled successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c488e4-4502-4bbb-98a7-3a2f4dffe51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b378093-cec2-46a1-b50f-43914a0e08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the emotion labels for voice\n",
    "voice_emotion_labels = {\n",
    "    0: 'neutral',\n",
    "    1: 'calm',\n",
    "    2: 'happy',\n",
    "    3: 'sad',\n",
    "    4: 'angry',\n",
    "    5: 'fearful',\n",
    "    6: 'disgust',\n",
    "    7: 'surprised'\n",
    "}\n",
    "\n",
    "# Define the emotion labels for face\n",
    "face_emotion_labels = {\n",
    "    0: 'angry',\n",
    "    1: 'disgust',\n",
    "    2: 'fear',\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'surprise',\n",
    "    6: 'neutral'\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "duration = 3  # seconds\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Check if the model files exist\n",
    "if not os.path.exists('/content/voice_emotion_model.keras'):\n",
    "    raise FileNotFoundError(\"Voice emotion model file not found.\")\n",
    "if not os.path.exists('/content/face_emotion_model.h5'):\n",
    "    raise FileNotFoundError(\"Face emotion model file not found.\")\n",
    "\n",
    "# Load the saved models\n",
    "voice_model = tf.keras.models.load_model('/content/voice_emotion_model.keras')\n",
    "face_model = tf.keras.models.load_model('/content/face_emotion_model.h5')\n",
    "\n",
    "def predict_voice_emotion(audio_data, sample_rate):\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "    # Calculate the desired number of frames\n",
    "    desired_columns = 173  # This is based on the model's expected input\n",
    "\n",
    "    # Adjust padding or truncation to make sure we get exactly 173 columns (frames)\n",
    "    if mfccs.shape[1] < desired_columns:\n",
    "        padding_width = desired_columns - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, padding_width)), mode='constant')\n",
    "    elif mfccs.shape[1] > desired_columns:\n",
    "        mfccs = mfccs[:, :desired_columns]\n",
    "\n",
    "    # Reshape the MFCCs to match the input shape expected by the model (1, 173, 13)\n",
    "    mfccs = np.expand_dims(mfccs, axis=-1)  # Add channel dimension (1)\n",
    "    mfccs = np.expand_dims(mfccs, axis=0)  # Add batch dimension (1)\n",
    "\n",
    "    # Predict emotion\n",
    "    predictions = voice_model.predict(mfccs)\n",
    "    emotion_index = np.argmax(predictions)\n",
    "    return voice_emotion_labels[emotion_index]\n",
    "\n",
    "def predict_face_emotion(frame):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(48, 48))\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return \"No face detected\"\n",
    "\n",
    "    # Assume one face per frame for simplicity\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = gray[y:y+h, x:x+w]\n",
    "        face_img = cv2.resize(face_img, (48, 48))\n",
    "        face_img = face_img.astype('float32') / 255\n",
    "        face_img = np.expand_dims(face_img, axis=0)\n",
    "        face_img = np.expand_dims(face_img, axis=-1)\n",
    "\n",
    "        predictions = face_model.predict(face_img)\n",
    "        emotion_index = np.argmax(predictions)\n",
    "        return face_emotion_labels[emotion_index]\n",
    "\n",
    "def record_and_predict():\n",
    "    # Record audio\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(duration * 44100), samplerate=44100, channels=1, dtype='float32')\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    audio_data = audio_data.flatten()  # Flatten the audio data to 1D array\n",
    "    voice_emotion = predict_voice_emotion(audio_data, 44100)\n",
    "    print(f\"Detected Voice Emotion: {voice_emotion}\")\n",
    "\n",
    "    # Capture video frame\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if ret:\n",
    "        face_emotion = predict_face_emotion(frame)\n",
    "        print(f\"Detected Face Emotion: {face_emotion}\")\n",
    "\n",
    "    # Combine predictions\n",
    "    if voice_emotion == face_emotion:\n",
    "        final_emotion = voice_emotion\n",
    "    else:\n",
    "        final_emotion = f\"Voice: {voice_emotion}, Face: {face_emotion}\"\n",
    "\n",
    "    print(f\"Final Detected Emotion: {final_emotion}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"Press Enter to record and capture (or type 'q' to quit)... \")\n",
    "        if user_input.lower() == 'q':\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        record_and_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689af8c6-a01d-43a6-8cc6-feb9b766faf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a89f3c-e0c2-4bf4-b763-abd80d09ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "\n",
    "# Define the emotion labels for voice\n",
    "voice_emotion_labels = {\n",
    "    0: 'neutral',\n",
    "    1: 'calm',\n",
    "    2: 'happy',\n",
    "    3: 'sad',\n",
    "    4: 'angry',\n",
    "    5: 'fearful',\n",
    "    6: 'disgust',\n",
    "    7: 'surprised'\n",
    "}\n",
    "\n",
    "# Define the emotion labels for face\n",
    "face_emotion_labels = {\n",
    "    0: 'angry',\n",
    "    1: 'disgust',\n",
    "    2: 'fear',\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'surprise',\n",
    "    6: 'neutral'\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "duration = 3  # seconds\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Check if the model files exist\n",
    "if not os.path.exists('/content/voice_emotion_model.keras'):\n",
    "    raise FileNotFoundError(\"Voice emotion model file not found.\")\n",
    "if not os.path.exists('/content/face_emotion_model.h5'):\n",
    "    raise FileNotFoundError(\"Face emotion model file not found.\")\n",
    "\n",
    "# Load the saved models\n",
    "voice_model = tf.keras.models.load_model('/content/voice_emotion_model.keras')\n",
    "face_model = tf.keras.models.load_model('/content/face_emotion_model.h5')\n",
    "\n",
    "def predict_voice_emotion(audio_data, sample_rate):\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "    # Calculate the desired number of frames\n",
    "    desired_columns = 173  # This is based on the model's expected input\n",
    "\n",
    "    # Adjust padding or truncation to make sure we get exactly 173 columns (frames)\n",
    "    if mfccs.shape[1] < desired_columns:\n",
    "        padding_width = desired_columns - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, padding_width)), mode='constant')\n",
    "    elif mfccs.shape[1] > desired_columns:\n",
    "        mfccs = mfccs[:, :desired_columns]\n",
    "\n",
    "    # Reshape the MFCCs to match the input shape expected by the model (1, 173, 13)\n",
    "    mfccs = np.expand_dims(mfccs, axis=-1)  # Add channel dimension (1)\n",
    "    mfccs = np.expand_dims(mfccs, axis=0)  # Add batch dimension (1)\n",
    "\n",
    "    # Predict emotion\n",
    "    predictions = voice_model.predict(mfccs)\n",
    "    emotion_index = np.argmax(predictions)\n",
    "    return voice_emotion_labels[emotion_index]\n",
    "\n",
    "def predict_face_emotion(frame):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(48, 48))\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return \"No face detected\"\n",
    "\n",
    "    # Assume one face per frame for simplicity\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = gray[y:y+h, x:x+w]\n",
    "        face_img = cv2.resize(face_img, (48, 48))\n",
    "        face_img = face_img.astype('float32') / 255\n",
    "        face_img = np.expand_dims(face_img, axis=0)\n",
    "        face_img = np.expand_dims(face_img, axis=-1)\n",
    "\n",
    "        predictions = face_model.predict(face_img)\n",
    "        emotion_index = np.argmax(predictions)\n",
    "        return face_emotion_labels[emotion_index]\n",
    "\n",
    "def record_audio():\n",
    "    while True:\n",
    "        audio_data = sd.rec(int(duration * 44100), samplerate=44100, channels=1, dtype='float32')\n",
    "        sd.wait()  # Wait until the recording is finished\n",
    "        audio_data = audio_data.flatten()  # Flatten the audio data to 1D array\n",
    "        voice_emotion = predict_voice_emotion(audio_data, 44100)\n",
    "        print(f\"Detected Voice Emotion: {voice_emotion}\")\n",
    "        global voice_emotion_display\n",
    "        voice_emotion_display = voice_emotion\n",
    "\n",
    "def capture_video():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        face_emotion = predict_face_emotion(frame)\n",
    "        if face_emotion != \"No face detected\":\n",
    "            cv2.putText(frame, f\"Face Emotion: {face_emotion}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if voice_emotion_display:\n",
    "            cv2.putText(frame, f\"Voice Emotion: {voice_emotion_display}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Webcam', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    voice_emotion_display = None\n",
    "\n",
    "    audio_thread = threading.Thread(target=record_audio)\n",
    "    video_thread = threading.Thread(target=capture_video)\n",
    "\n",
    "    audio_thread.start()\n",
    "    video_thread.start()\n",
    "\n",
    "    audio_thread.join()\n",
    "    video_thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776940da-fb3e-4fa7-a163-557d60237647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d775e9-5fe0-43a1-871a-2a3d2fa5353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8333333333333334\n",
      "Precision: 0.9166666666666666\n",
      "Recall: 0.8333333333333334\n",
      "F1 Score: 0.8444444444444446\n",
      "Recording...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Detected Voice Emotion: disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Detected Face Emotion: happy\n",
      "Final Emotion: happy\n",
      "Recommended Music: Happy Song 1\n",
      "Recommended Joke: Happy Joke 1\n",
      "Recommended Deep Breathe: Deep Breathe Exercise 2\n",
      "Recommended Book: Book 1\n",
      "Recommended Yoga: Yoga 2\n",
      "Recommended Video: Video 1\n",
      "Recommended Puzzle: Puzzle 3\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the emotion labels for voice and face\n",
    "voice_emotion_labels = {\n",
    "    0: 'neutral',\n",
    "    1: 'calm',\n",
    "    2: 'happy',\n",
    "    3: 'sad',\n",
    "    4: 'angry',\n",
    "    5: 'fearful',\n",
    "    6: 'disgust',\n",
    "    7: 'surprised'\n",
    "}\n",
    "\n",
    "face_emotion_labels = {\n",
    "    0: 'angry',\n",
    "    1: 'disgust',\n",
    "    2: 'fear',\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'surprise',\n",
    "    6: 'neutral'\n",
    "}\n",
    "\n",
    "# Predefined recommendations\n",
    "recommendations = {\n",
    "    'music': {\n",
    "        'happy': [\"Happy Song 1\", \"Happy Song 2\", \"Happy Song 3\"],\n",
    "        'sad': [\"Uplifting Song 1\", \"Uplifting Song 2\", \"Uplifting Song 3\"],\n",
    "        'fear': [\"Calm Song 1\", \"Calm Song 2\", \"Calm Song 3\"],\n",
    "        'disgust': [\"Relaxing Song 1\", \"Relaxing Song 2\", \"Relaxing Song 3\"]\n",
    "    },\n",
    "    'jokes': {\n",
    "        'happy': [\"Happy Joke 1\", \"Happy Joke 2\", \"Happy Joke 3\"],\n",
    "        'sad': [\"Cheerful Joke 1\", \"Cheerful Joke 2\", \"Cheerful Joke 3\"],\n",
    "        'fear': [\"Funny Joke 1\", \"Funny Joke 2\", \"Funny Joke 3\"],\n",
    "        'disgust': [\"Silly Joke 1\", \"Silly Joke 2\", \"Silly Joke 3\"]\n",
    "    },\n",
    "    'deep_breathe': {\n",
    "        'happy': [\"Deep Breathe Exercise 1\", \"Deep Breathe Exercise 2\", \"Deep Breathe Exercise 3\"],\n",
    "        'sad': [\"Deep Breathe Exercise 4\", \"Deep Breathe Exercise 5\", \"Deep Breathe Exercise 6\"],\n",
    "        'fear': [\"Deep Breathe Exercise 7\", \"Deep Breathe Exercise 8\", \"Deep Breathe Exercise 9\"],\n",
    "        'disgust': [\"Deep Breathe Exercise 10\", \"Deep Breathe Exercise 11\", \"Deep Breathe Exercise 12\"]\n",
    "    },\n",
    "    'books': {\n",
    "        'happy': [\"Book 1\", \"Book 2\", \"Book 3\"],\n",
    "        'sad': [\"Book 4\", \"Book 5\", \"Book 6\"],\n",
    "        'fear': [\"Book 7\", \"Book 8\", \"Book 9\"],\n",
    "        'disgust': [\"Book 10\", \"Book 11\", \"Book 12\"]\n",
    "    },\n",
    "    'yoga': {\n",
    "        'happy': [\"Yoga 1\", \"Yoga 2\", \"Yoga 3\"],\n",
    "        'sad': [\"Yoga 4\", \"Yoga 5\", \"Yoga 6\"],\n",
    "        'fear': [\"Yoga 7\", \"Yoga 8\", \"Yoga 9\"],\n",
    "        'disgust': [\"Yoga 10\", \"Yoga 11\", \"Yoga 12\"]\n",
    "    },\n",
    "    'videos': {\n",
    "        'happy': [\"Video 1\", \"Video 2\", \"Video 3\"],\n",
    "        'sad': [\"Video 4\", \"Video 5\", \"Video 6\"],\n",
    "        'fear': [\"Video 7\", \"Video 8\", \"Video 9\"],\n",
    "        'disgust': [\"Video 10\", \"Video 11\", \"Video 12\"]\n",
    "    },\n",
    "    'puzzles': {\n",
    "        'happy': [\"Puzzle 1\", \"Puzzle 2\", \"Puzzle 3\"],\n",
    "        'sad': [\"Puzzle 4\", \"Puzzle 5\", \"Puzzle 6\"],\n",
    "        'fear': [\"Puzzle 7\", \"Puzzle 8\", \"Puzzle 9\"],\n",
    "        'disgust': [\"Puzzle 10\", \"Puzzle 11\", \"Puzzle 12\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def recommend_content(emotion):\n",
    "    content = {}\n",
    "    if emotion in recommendations['music']:\n",
    "        content['music'] = random.choice(recommendations['music'][emotion])\n",
    "        content['jokes'] = random.choice(recommendations['jokes'][emotion])\n",
    "        content['deep_breathe'] = random.choice(recommendations['deep_breathe'][emotion])\n",
    "        content['books'] = random.choice(recommendations['books'][emotion])\n",
    "        content['yoga'] = random.choice(recommendations['yoga'][emotion])\n",
    "        content['videos'] = random.choice(recommendations['videos'][emotion])\n",
    "        content['puzzles'] = random.choice(recommendations['puzzles'][emotion])\n",
    "    return content\n",
    "\n",
    "# Parameters\n",
    "duration = 3  # seconds\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Check if the model files exist\n",
    "if not os.path.exists('/content/voice_emotion_model.keras'):\n",
    "    raise FileNotFoundError(\"Voice emotion model file not found.\")\n",
    "if not os.path.exists('/content/face_emotion_model.h5'):\n",
    "    raise FileNotFoundError(\"Face emotion model file not found.\")\n",
    "\n",
    "# Load the saved models\n",
    "voice_model = tf.keras.models.load_model('/content/voice_emotion_model.keras')\n",
    "face_model = tf.keras.models.load_model('/content/face_emotion_model.h5')\n",
    "\n",
    "def predict_voice_emotion(audio_data, sample_rate):\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "    # Calculate the desired number of frames\n",
    "    desired_columns = 173  # This is based on the model's expected input\n",
    "\n",
    "    # Adjust padding or truncation to make sure we get exactly 173 columns (frames)\n",
    "    if mfccs.shape[1] < desired_columns:\n",
    "        padding_width = desired_columns - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, padding_width)), mode='constant')\n",
    "    elif mfccs.shape[1] > desired_columns:\n",
    "        mfccs = mfccs[:, :desired_columns]\n",
    "\n",
    "    # Reshape the MFCCs to match the input shape expected by the model (1, 173, 13)\n",
    "    mfccs = np.expand_dims(mfccs, axis=-1)  # Add channel dimension (1)\n",
    "    mfccs = np.expand_dims(mfccs, axis=0)  # Add batch dimension (1)\n",
    "\n",
    "    # Predict emotion\n",
    "    predictions = voice_model.predict(mfccs)\n",
    "    emotion_index = np.argmax(predictions)\n",
    "    return voice_emotion_labels[emotion_index]\n",
    "\n",
    "def predict_face_emotion(frame):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(48, 48))\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return \"No face detected\"\n",
    "\n",
    "    # Assume one face per frame for simplicity\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = gray[y:y+h, x:x+w]\n",
    "        face_img = cv2.resize(face_img, (48, 48))\n",
    "        face_img = face_img.astype('float32') / 255\n",
    "        face_img = np.expand_dims(face_img, axis=0)\n",
    "        face_img = np.expand_dims(face_img, axis=-1)\n",
    "\n",
    "        predictions = face_model.predict(face_img)\n",
    "        emotion_index = np.argmax(predictions)\n",
    "        return face_emotion_labels[emotion_index]\n",
    "\n",
    "def record_and_predict():\n",
    "    # Record audio\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(duration * 44100), samplerate=44100, channels=1, dtype='float32')\n",
    "    sd.wait()  # Wait until the recording is finished\n",
    "    audio_data = audio_data.flatten()  # Flatten the audio data to 1D array\n",
    "    voice_emotion = predict_voice_emotion(audio_data, 44100)\n",
    "    print(f\"Detected Voice Emotion: {voice_emotion}\")\n",
    "\n",
    "    # Capture video frame\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        face_emotion = predict_face_emotion(frame)\n",
    "        print(f\"Detected Face Emotion: {face_emotion}\")\n",
    "\n",
    "        # Display recommendations based on combined emotion\n",
    "        final_emotion = face_emotion if face_emotion != \"No face detected\" else voice_emotion\n",
    "        content = recommend_content(final_emotion)\n",
    "        display_recommendations(content, frame, face_emotion, voice_emotion)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def display_recommendations(content, frame, face_emotion, voice_emotion):\n",
    "    print(f\"Final Emotion: {face_emotion if face_emotion != 'No face detected' else voice_emotion}\")\n",
    "    print(f\"Recommended Music: {content.get('music', 'No recommendation')}\")\n",
    "    print(f\"Recommended Joke: {content.get('jokes', 'No recommendation')}\")\n",
    "    print(f\"Recommended Deep Breathe: {content.get('deep_breathe', 'No recommendation')}\")\n",
    "    print(f\"Recommended Book: {content.get('books', 'No recommendation')}\")\n",
    "    print(f\"Recommended Yoga: {content.get('yoga', 'No recommendation')}\")\n",
    "    print(f\"Recommended Video: {content.get('videos', 'No recommendation')}\")\n",
    "    print(f\"Recommended Puzzle: {content.get('puzzles', 'No recommendation')}\")\n",
    "\n",
    "# For demonstration purposes, we'll create some dummy data\n",
    "true_emotions = ['happy', 'sad', 'fear', 'happy', 'disgust', 'happy']\n",
    "predicted_emotions = ['happy', 'sad', 'fear', 'happy', 'disgust', 'sad']\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(true_emotions, predicted_emotions)\n",
    "precision = precision_score(true_emotions, predicted_emotions, average='weighted')\n",
    "recall = recall_score(true_emotions, predicted_emotions, average='weighted')\n",
    "f1 = f1_score(true_emotions, predicted_emotions, average='weighted')\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Run the system\n",
    "record_and_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b979b2-9af3-4c1e-b62d-43767e9deed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e58ab1-d147-479e-badb-20b85934060a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
